{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "%pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\personal_files\\\\python\\\\end_to_End_nlp_projects\\\\1_text_summarization\\\\__github__\\\\text-summarization\\\\research'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "os.chdir(\"../\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\personal_files\\\\python\\\\end_to_End_nlp_projects\\\\1_text_summarization\\\\__github__\\\\text-summarization'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from dataclasses import dataclass\r\n",
    "from pathlib import Path\r\n",
    "@dataclass(frozen=True)\r\n",
    "class ModelEvaluationConfig:\r\n",
    "    root_dir: Path\r\n",
    "    data_path: Path\r\n",
    "    model_path: Path\r\n",
    "    tokenizer_path: Path\r\n",
    "    metric_file_name: Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from textsummarization import constants\r\n",
    "from textsummarization.utils.common import read_yaml, create_directories"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ConfigurationManager:\r\n",
    "    def __init__(self, config_file_path = constants.CONFIG_FILE_PATH, params_file_path = constants.PARAMS_FILE_PATH):\r\n",
    "        self.config = read_yaml(config_file_path)\r\n",
    "        self.params = read_yaml(params_file_path)\r\n",
    "        create_directories([self.config.artifacts_roots])\r\n",
    "    \r\n",
    "    def get_model_evaluation_config(self, ):\r\n",
    "        config = self.config.model_evaluation\r\n",
    "        create_directories([config.root_dir])\r\n",
    "        model_evaluation_config = ModelEvaluationConfig(\r\n",
    "            root_dir=config.root_dir, data_path=config.data_path, model_path=config.model_path, \r\n",
    "            tokenizer_path=config.tokenizer_path, metric_file_name=config.metric_file_name\r\n",
    "        )\r\n",
    "        return model_evaluation_config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\r\n",
    "from datasets import load_dataset, load_from_disk, load_metric\r\n",
    "import torch\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class ModelEvaluation:\r\n",
    "    def __init__(self, config: ModelEvaluationConfig):\r\n",
    "        self.config = config\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\r\n",
    "        \"\"\"split the dataset into smaller batches that we can process simultaneously\r\n",
    "        Yield successive batch-sized chunks from list_of_elements.\"\"\"\r\n",
    "        for i in range(0, len(list_of_elements), batch_size):\r\n",
    "            yield list_of_elements[i : i + batch_size]\r\n",
    "\r\n",
    "    \r\n",
    "    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \r\n",
    "                               batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \r\n",
    "                               column_text=\"article\", \r\n",
    "                               column_summary=\"highlights\"):\r\n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\r\n",
    "        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\r\n",
    "\r\n",
    "        for article_batch, target_batch in tqdm(\r\n",
    "            zip(article_batches, target_batches), total=len(article_batches)):\r\n",
    "            \r\n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \r\n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\r\n",
    "            \r\n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\r\n",
    "                            attention_mask=inputs[\"attention_mask\"].to(device), \r\n",
    "                            length_penalty=0.8, num_beams=8, max_length=128)\r\n",
    "            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\r\n",
    "            \r\n",
    "            # Finally, we decode the generated texts, \r\n",
    "            # replace the  token, and add the decoded texts with the references to the metric.\r\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \r\n",
    "                                    clean_up_tokenization_spaces=True) \r\n",
    "                for s in summaries]      \r\n",
    "            \r\n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\r\n",
    "            \r\n",
    "            \r\n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\r\n",
    "            \r\n",
    "        #  Finally compute and return the ROUGE scores.\r\n",
    "        score = metric.compute()\r\n",
    "        return score\r\n",
    "\r\n",
    "\r\n",
    "    def evaluate(self):\r\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\r\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\r\n",
    "       \r\n",
    "        #loading data \r\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\r\n",
    "\r\n",
    "\r\n",
    "        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\r\n",
    "  \r\n",
    "        rouge_metric = load_metric('rouge')\r\n",
    "\r\n",
    "        score = self.calculate_metric_on_test_ds(\r\n",
    "        dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\r\n",
    "            )\r\n",
    "\r\n",
    "        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\r\n",
    "\r\n",
    "        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )\r\n",
    "        df.to_csv(self.config.metric_file_name, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "try:\r\n",
    "    config = ConfigurationManager()\r\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\r\n",
    "    model_evaluation_config = ModelEvaluation(config = model_evaluation_config)\r\n",
    "    model_evaluation_config.evaluate()\r\n",
    "except Exception as e:\r\n",
    "    raise e"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2023-07-09 11:30:12,678: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-07-09 11:30:12,679: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-07-09 11:30:12,681: INFO: common: Created directory at: artifacts]\n",
      "[2023-07-09 11:30:12,683: INFO: common: Created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['length_penality'] (note: typos in the generate arguments will also show up in this list)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     model_evaluation_config\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     model_evaluation_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_model_evaluation_config()\n\u001b[0;32m      4\u001b[0m     model_evaluation_config \u001b[38;5;241m=\u001b[39m ModelEvaluation(config \u001b[38;5;241m=\u001b[39m model_evaluation_config)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmodel_evaluation_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[18], line 35\u001b[0m, in \u001b[0;36mModelEvaluation.evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m rouge_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrougeL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrougeLsum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     33\u001b[0m rouge_metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_metric_on_test_ds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_samsum_pt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouge_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_pegasus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdialogue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_summary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m rouge_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((rn, score[rn]\u001b[38;5;241m.\u001b[39mmid\u001b[38;5;241m.\u001b[39mfmeasure) \u001b[38;5;28;01mfor\u001b[39;00m rn \u001b[38;5;129;01min\u001b[39;00m rouge_names)\n\u001b[0;32m     41\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rouge_dict, index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mModelEvaluation.calculate_metric_on_test_ds\u001b[1;34m(self, dataset, metric, model, tokenizer, batch_size, device, column_text, column_summary)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(article_batches, target_batches), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(article_batches)):\n\u001b[0;32m     17\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(article_batch, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m, truncation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m---> 18\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlength_penality\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     decoded_summaries \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(s, skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m summaries]\n\u001b[0;32m     20\u001b[0m     decoded_summaries \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m decoded_summaries]\n",
      "File \u001b[1;32md:\\personal_files\\python\\end_to_End_nlp_projects\\1_text_summarization\\__github__\\text-summarization\\myenv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\personal_files\\python\\end_to_End_nlp_projects\\1_text_summarization\\__github__\\text-summarization\\myenv\\lib\\site-packages\\transformers\\generation\\utils.py:1271\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1269\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m-> 1271\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[1;32md:\\personal_files\\python\\end_to_End_nlp_projects\\1_text_summarization\\__github__\\text-summarization\\myenv\\lib\\site-packages\\transformers\\generation\\utils.py:1144\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[1;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1147\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['length_penality'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('myenv': venv)"
  },
  "interpreter": {
   "hash": "4464cb5e0033a3ea7e76d137774f8f4d1b617d1887eaaaa3370170b8dddeb4e7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}